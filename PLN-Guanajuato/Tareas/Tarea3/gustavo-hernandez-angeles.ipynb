{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Procesamiento de Lenguaje Natural**\n",
    "## **Tarea 3 - Minería de Texto Básica**\n",
    "## Gustavo Hernández Angeles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Selección de Términos y DTR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1\n",
    "Programa y visualiza TCOR. Puede hacer esto de forma similar a como el profesor lo hizo en la *Práctica 4* con DOR. El pesado puede ser el que el profesor sugirió en clase TCOR o PPMI como lo sugiere Dan Jurafsky: hacer al menos dos gráficas; la de constelación de palabras y subconjunto para ver algunas palabras con flechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "\n",
    "    # Manera más chida de abrir (y cerrar auto) archivos.\n",
    "    with open(path_corpus, \"r\", encoding=\"utf-8-sig\") as f_corpus, open(path_truth, \"r\", encoding=\"utf-8-sig\") as f_truth:\n",
    "        for twitt in f_corpus:\n",
    "            tr_txt += [twitt]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label]\n",
    "\n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_data = Path(\"./data/mex_train.txt\")\n",
    "path_train_labels = Path(\"./data/mex_train_labels.txt\")\n",
    "tr_txt, tr_y = get_texts_from_file(path_train_data, path_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_palabras = []\n",
    "for doc in tr_txt:\n",
    "    corpus_palabras += tokenizer.tokenize(doc)\n",
    "fdist = nltk.FreqDist(corpus_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos las 5000 palabras más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux\n",
    "\n",
    "V = sortFreqDict(fdist)\n",
    "V = V[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indices = dict()\n",
    "contador=0\n",
    "for weight, word in V:\n",
    "    dict_indices[word] = contador\n",
    "    contador += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero haremos la BoW de frecuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una BOW con esquema de peso frecuencia.\n",
    "def build_bow_tr(tr_txt, V, dict_indices):\n",
    "    BOW = np.zeros(shape=(len(tr_txt), len(V)), dtype=int)\n",
    "    \n",
    "    \n",
    "    cont_doc = 0\n",
    "    for tr in tr_txt:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        \n",
    "        for word in fdist_doc:\n",
    "            if word not in dict_indices:\n",
    "                continue\n",
    "            BOW[cont_doc, dict_indices[word]] += 1\n",
    "            \n",
    "        cont_doc += 1\n",
    "    return BOW\n",
    "\n",
    "BOW_frecuencia_tr = build_bow_tr(tr_txt, V, dict_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculamos TCOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tcor(BOW : np.ndarray):\n",
    "    tam_V = BOW.shape[1] # Tamaño de vocabulario\n",
    "    \n",
    "    TCOR = np.zeros(shape=(tam_V, tam_V), dtype=float)\n",
    "    print(\"DOR: Shape of the input matrix (BoT):\", BOW.shape)\n",
    "    print(\"DOR: Shape of the term-feat matrix:\", TCOR.shape)\n",
    "    \n",
    "    \n",
    "       \n",
    "    return TCOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2\n",
    "Programa y visualiza alguna implementación de Random Indexing. Puedes hacer esto \n",
    "reasando parte del código del profesor en la *Práctica 4* con DOR. Hacer al menos dos \n",
    "gráficas, la de constelación de palabras y subconjunto para ver algunas palabras con \n",
    "flechas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3\n",
    "Use alguna de las DTRs anteriores por separado de alguna forma para clasificación de \n",
    "documentos (e.g., promedio de vectores de términos en cada documento para representar). \n",
    "Compárelas contra un BoW-TFIDF de 5000 palabras más frecuentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4\n",
    "Bajo la representación TCOR de los términos, y asumiendo un vocabulario de 5000 \n",
    "palabras, muestre por orden de mayor similitud coseno (ver chp 6 del libro de Dan) los \n",
    "10 pares de palabras más parecidas en toda la colección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5\n",
    "Bajo alguna representación de documentos hecha en esta tarea, y asumiendo un vocabu-\n",
    "lario de 5000 palabras, muestre por orden de más similitud coseno (ver chp 6 del libro \n",
    "de Dan) 10 pares de documentos más parecidos en toda la colección (no muestres pares \n",
    "de documentos que contengan exactamente substrings uno del otro; pues hay algo de \n",
    "redundancia en los datos). Muestre el texto que contienen y muestre la categoría de cada \n",
    "uno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6\n",
    "Implemente Ganancia de Información o Chi2 como lo sugiere Baeza-Yates (no función \n",
    "de sklearn ni similar) para descubrir el top 50 de las palabras más relevantes de TODA la \n",
    "colección. Haga una gráfica también con la herramienta de *word_cloud* dónde el tamaño \n",
    "de la palabra corresponda a su ganancia de información o Chi2:\n",
    "\n",
    "- https://amueller.github.io/word_cloud/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
