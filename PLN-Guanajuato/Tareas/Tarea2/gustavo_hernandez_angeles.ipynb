{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 - NLP CIMAT: Minería de Texto Básica\n",
    "## Por: Gustavo Hernández Angeles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación (Lectura de corpus, tokenización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leyendo corpus\n",
    "* Utilizaré parte del código visto en la práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_labels):\n",
    "    tr_txt = []\n",
    "    tr_y = []\n",
    "\n",
    "    with open(path_corpus, \"r\", encoding=\"utf-8-sig\") as f_corpus, open(path_labels, \"r\", encoding=\"utf-8-sig\") as f_truth:\n",
    "        for twitt in f_corpus:\n",
    "            tr_txt += [twitt.strip()]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label.strip()]\n",
    "\n",
    "    return tr_txt, tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"./data/mex_train.txt\", \"./data/mex_train_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_txt, val_y = get_texts_from_file(\"./data/mex_val.txt\", \"./data/mex_val_labels.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizacion de Corpus  \n",
    "Haré que solo lea las palabras y quitaré los stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-ZáéíóúñÁÉÍÓÚÑ]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 12161\n"
     ]
    }
   ],
   "source": [
    "stopwords_espanol = stopwords.words(\"spanish\")\n",
    "corpus_palabras = []\n",
    "for doc in tr_txt:\n",
    "    text = tokenizer.tokenize(doc)\n",
    "    corpus_palabras += [w.lower() for w in text if w.lower() not in stopwords_espanol]\n",
    "\n",
    "fdist = nltk.FreqDist(corpus_palabras)\n",
    "print(f\"Tamaño del vocabulario: {len(fdist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se hace un filtrado por frecuencia, al hacer esto, reducimos el tamaño del vocabulario de 14268 a 4683  \n",
    "Se escoge un umbral de 1, valor mayor reduce más el vocabulario.  \n",
    "Con 2 -> el tamaño de V es 2749  \n",
    "Con 3 -> el tamaño de V es 1973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 4281\n"
     ]
    }
   ],
   "source": [
    "K_umbral = 1\n",
    "V = [(fdist[key], key) for key in fdist if fdist[key] > K_umbral] # Más de una ocurrencia\n",
    "V.sort()\n",
    "V.reverse()\n",
    "print(f\"Tamaño del vocabulario: {len(V)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indices = dict()\n",
    "cont = 0\n",
    "for freq, word in V:\n",
    "    dict_indices[word] = cont\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bolsas de Palabras, Bigramas y Emociones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Evalúe BOW con pesado binario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definimos una función para realizar el BOW con peso binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binario_bow(tr_txt, V, dict_indices):\n",
    "    BOW = np.zeros(shape=(len(tr_txt), len(V)), dtype=int)\n",
    "    \n",
    "    cont_doc = 0\n",
    "    for tr in tr_txt:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        \n",
    "        for word in fdist_doc:\n",
    "            # Se elige ignorar si un término no está en el vocab.\n",
    "            if word not in dict_indices:\n",
    "                continue\n",
    "            BOW[cont_doc, dict_indices[word]] = 1\n",
    "            \n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definimos una función para evaluar el SVM dados las BOW de train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_bow(BOW_train, BOW_test, val_y):\n",
    "    # Parámetro de complejidad del SVM, se proponen estos\n",
    "    # y se recorrerán con GridSearch\n",
    "    parameters = {\"C\": [.05, .12, .25, .5, 1, 2, 4]}\n",
    "    # Tratar de penalizar con base a la proporción de ejemplos\n",
    "    # en cada clase\n",
    "    svr = svm.LinearSVC(class_weight='balanced', max_iter=10000)\n",
    "    grid = GridSearchCV(estimator=svr, param_grid=parameters,\n",
    "                        n_jobs=6, scoring=\"f1_macro\", cv=5)\n",
    "    grid.fit(BOW_train, tr_y)\n",
    "    y_pred = grid.predict(BOW_test)\n",
    "\n",
    "    print(confusion_matrix(val_y, y_pred))\n",
    "    print(metrics.classification_report(val_y, y_pred))\n",
    "    print(f\"F1-score: {f1_score(val_y, y_pred, pos_label='1'):.4f}\")\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluamos con pesado binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[363  55]\n",
      " [ 50 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87       418\n",
      "           1       0.68      0.70      0.69       169\n",
      "\n",
      "    accuracy                           0.82       587\n",
      "   macro avg       0.78      0.79      0.78       587\n",
      "weighted avg       0.82      0.82      0.82       587\n",
      "\n",
      "F1-score: 0.6939\n"
     ]
    }
   ],
   "source": [
    "BOW_train = binario_bow(tr_txt, V, dict_indices)\n",
    "BOW_test = binario_bow(val_txt, V, dict_indices)\n",
    "svm_binario = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Evalúe BOW con pesado frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos BOW con esquema de peso de frecuencia.\n",
    "def frecuencia_bow(tr_txt, V, dict_indices):\n",
    "    BOW = np.zeros(shape=(len(tr_txt), len(V)), dtype=int)\n",
    "    \n",
    "    cont_doc = 0\n",
    "    for tr in tr_txt:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        \n",
    "        for word, freq in fdist_doc.items():\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc, dict_indices[word]] = freq\n",
    "        cont_doc += 1\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluamos con pesado frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[358  60]\n",
      " [ 51 118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       418\n",
      "           1       0.66      0.70      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.77       587\n",
      "weighted avg       0.81      0.81      0.81       587\n",
      "\n",
      "F1-score: 0.6801\n"
     ]
    }
   ],
   "source": [
    "BOW_train = frecuencia_bow(tr_txt, V, dict_indices)\n",
    "BOW_test = frecuencia_bow(val_txt, V, dict_indices)\n",
    "svm_frecuencia = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Evalúe BOW con pesado tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos BOW con esquema de peso de tf-idf\n",
    "def tfidf_bow(tr_txt, V, dict_indices):\n",
    "\n",
    "    \n",
    "    # Un BOW de frecuencia nos ayuda a obtener tf\n",
    "    # Lo haremos logaritmico\n",
    "    f_bow = frecuencia_bow(tr_txt, V, dict_indices)\n",
    "    tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n",
    "    \n",
    "    # De misma forma para df\n",
    "    n_docs = len(tr_txt)\n",
    "    df = np.count_nonzero(f_bow, axis=0)\n",
    "    idf = np.log10(n_docs / (1+df))\n",
    "    \n",
    "    BOW = tf*idf\n",
    "\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus\\AppData\\Local\\Temp\\ipykernel_12236\\143517032.py:8: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[351  67]\n",
      " [ 57 112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       418\n",
      "           1       0.63      0.66      0.64       169\n",
      "\n",
      "    accuracy                           0.79       587\n",
      "   macro avg       0.74      0.75      0.75       587\n",
      "weighted avg       0.79      0.79      0.79       587\n",
      "\n",
      "F1-score: 0.6437\n"
     ]
    }
   ],
   "source": [
    "BOW_train = tfidf_bow(tr_txt, V, dict_indices)\n",
    "BOW_test = tfidf_bow(val_txt, V, dict_indices)\n",
    "svm_tfidf = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Evalúe BOW con pesado binario normalizado l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear BOW con pesado binario normalizado l2 (norma euclidiana)\n",
    "def binarionorm_bow(tr_txt, V, dict_indices):\n",
    "    BOW = binario_bow(tr_txt, V, dict_indices)\n",
    "    norma = np.linalg.norm(BOW, axis=1, keepdims=True)\n",
    "    norma[norma==0] = 1 # Evitar div por 0\n",
    "    return BOW / norma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[355  63]\n",
      " [ 50 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86       418\n",
      "           1       0.65      0.70      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.77       587\n",
      "weighted avg       0.81      0.81      0.81       587\n",
      "\n",
      "F1-score: 0.6781\n"
     ]
    }
   ],
   "source": [
    "BOW_train = binarionorm_bow(tr_txt, V, dict_indices)\n",
    "BOW_test = binarionorm_bow(val_txt, V, dict_indices)\n",
    "svm_binnorm = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Evalúe BOW con pesado frecuencia normalizado l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función BOW pesado frecuencia normalizado l2\n",
    "def frecuencianorm_bow(tr_txt, V, dict_indices):\n",
    "    BOW = frecuencia_bow(tr_txt, V, dict_indices)\n",
    "    norma = np.linalg.norm(BOW, axis=1, keepdims=True)\n",
    "    norma[norma==0] = 1\n",
    "    return BOW/norma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[351  67]\n",
      " [ 47 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86       418\n",
      "           1       0.65      0.72      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.76      0.78      0.77       587\n",
      "weighted avg       0.81      0.81      0.81       587\n",
      "\n",
      "F1-score: 0.6816\n"
     ]
    }
   ],
   "source": [
    "BOW_train = frecuencianorm_bow(tr_txt, V, dict_indices)\n",
    "BOW_test = frecuencianorm_bow(val_txt, V, dict_indices)\n",
    "svm_frecnorm = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Evalúe BOW con pesado tfidf normalizado l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfnorm_bow(tr_txt, V, dict_indices):\n",
    "    BOW = tfidf_bow(tr_txt, V, dict_indices)\n",
    "    norma = np.linalg.norm(BOW, axis=1, keepdims=True)\n",
    "    norma[norma==0] = 1\n",
    "    return BOW/norma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus\\AppData\\Local\\Temp\\ipykernel_12236\\143517032.py:8: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[358  60]\n",
      " [ 50 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       418\n",
      "           1       0.66      0.70      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.78       587\n",
      "weighted avg       0.82      0.81      0.81       587\n",
      "\n",
      "F1-score: 0.6839\n"
     ]
    }
   ],
   "source": [
    "BOW_train = tfidfnorm_bow(tr_txt, V, dict_indices)\n",
    "BOW_test = tfidfnorm_bow(val_txt, V, dict_indices)\n",
    "svm_tfidfnorm = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Ponga una tabla comparativa a modo de resumen con las seis entradas anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representación  | Macro F1\n",
    "----------------|----------\n",
    "Binario         | **69.39%**\n",
    "Frecuencia      | 68.01%\n",
    "TF-IDF          | 64.37%\n",
    "Binario L2      | 67.81%\n",
    "Frecuencia L2   | 68.16%\n",
    "TF-IDF L2       | 68.39%\n",
    "\n",
    "Aún cuando el esquema de pesado TF-IDF puede tener en cuenta más aspectos de los términos, en este problema arroja un score inferior al binario, el cual es muy simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. De las configuraciones anteriores eliga la mejor y evalúela con más y menos términos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Todo el vocabulario con al menos dos ocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 4281\n",
      "Tamaño del diccionario: 4281\n"
     ]
    }
   ],
   "source": [
    "V = [(fdist[key], key) for key in fdist if fdist[key] > 1]\n",
    "V.sort()\n",
    "V.reverse()\n",
    "longitud_vocabulario = len(V)\n",
    "print(f\"Tamaño del vocabulario: {longitud_vocabulario}\")\n",
    "\n",
    "dict_indices = dict()\n",
    "cont = 0\n",
    "for freq, word in V:\n",
    "    dict_indices[word] = cont\n",
    "    cont += 1\n",
    "print(f\"Tamaño del diccionario: {len(dict_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[363  55]\n",
      " [ 50 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87       418\n",
      "           1       0.68      0.70      0.69       169\n",
      "\n",
      "    accuracy                           0.82       587\n",
      "   macro avg       0.78      0.79      0.78       587\n",
      "weighted avg       0.82      0.82      0.82       587\n",
      "\n",
      "F1-score: 0.6939\n"
     ]
    }
   ],
   "source": [
    "BOW_train = binario_bow(tr_txt, V, dict_indices)\n",
    "BOW_test = binario_bow(val_txt, V, dict_indices)\n",
    "svm_masPalabras = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mitad del Vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 2140\n",
      "Tamaño del diccionario: 2140\n"
     ]
    }
   ],
   "source": [
    "V_mitad = V[:int(longitud_vocabulario/2)]\n",
    "dict_indices = dict()\n",
    "cont = 0\n",
    "for freq, word in V_mitad:\n",
    "    dict_indices[word] = cont\n",
    "    cont += 1\n",
    "print(f\"Tamaño del vocabulario: {len(V_mitad)}\")\n",
    "print(f\"Tamaño del diccionario: {len(dict_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[360  58]\n",
      " [ 47 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       418\n",
      "           1       0.68      0.72      0.70       169\n",
      "\n",
      "    accuracy                           0.82       587\n",
      "   macro avg       0.78      0.79      0.79       587\n",
      "weighted avg       0.82      0.82      0.82       587\n",
      "\n",
      "F1-score: 0.6991\n"
     ]
    }
   ],
   "source": [
    "BOW_train = binario_bow(tr_txt, V_mitad, dict_indices)\n",
    "BOW_test = binario_bow(val_txt, V_mitad, dict_indices)\n",
    "svm_mitadPalabras = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 1000\n",
      "Tamaño del diccionario: 1000\n"
     ]
    }
   ],
   "source": [
    "V_1000 = V[:1000]\n",
    "dict_indices = dict()\n",
    "cont = 0\n",
    "for freq, word in V_1000:\n",
    "    dict_indices[word] = cont\n",
    "    cont += 1\n",
    "print(f\"Tamaño del vocabulario: {len(V_1000)}\")\n",
    "print(f\"Tamaño del diccionario: {len(dict_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[358  60]\n",
      " [ 51 118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       418\n",
      "           1       0.66      0.70      0.68       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.77       587\n",
      "weighted avg       0.81      0.81      0.81       587\n",
      "\n",
      "F1-score: 0.6801\n"
     ]
    }
   ],
   "source": [
    "BOW_train = binario_bow(tr_txt, V_mitad, dict_indices)\n",
    "BOW_test = binario_bow(val_txt, V_mitad, dict_indices)\n",
    "svm_1000 = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabla comparativa de binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Términos utilizados  | Macro F1\n",
    "----------------|----------\n",
    "4281         | 69.39%\n",
    "2140      | **69.91%**\n",
    "1000          | 68.01%\n",
    "\n",
    "Resultó mejor utiilzar la mitad de las palabras con una ventaja de +0.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Recurso Léxico EmoLex para construir una Bolsa de Emociones (BoE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "emolex_path = \"./data/Spanish-NRC-EmoLex.txt\"\n",
    "emolex = pd.read_csv(emolex_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recurso que encontré se encuentra en `./data/Spanish-NRC-EmoLex.txt`. Inspeccionandolo encuentro que:\n",
    "* Cada renglón es una palabra, al que le corresponde un *vector de emociones* incluyendo el sentimiento **positivo** o **negativo**.\n",
    "* Las palabras provienen del inglés y son traducidas al español, por lo que puede haber palabras repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "English Word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "anger",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "anticipation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "disgust",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fear",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "joy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "negative",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "positive",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sadness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "surprise",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trust",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Spanish Word",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0ac05540-3c17-4273-b00a-8220de8b84af",
       "rows": [
        [
         "0",
         "aback",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "detrás"
        ],
        [
         "1",
         "abacus",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "ábaco"
        ],
        [
         "2",
         "abandon",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "abandonar"
        ],
        [
         "3",
         "abandoned",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "abandonado"
        ],
        [
         "4",
         "abandonment",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "1",
         "0",
         "abandono"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English Word</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>Spanish Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>detrás</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ábaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>abandonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>abandonado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>abandono</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Word  anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
       "0        aback      0             0        0     0    0         0         0   \n",
       "1       abacus      0             0        0     0    0         0         0   \n",
       "2      abandon      0             0        0     1    0         1         0   \n",
       "3    abandoned      1             0        0     1    0         1         0   \n",
       "4  abandonment      1             0        0     1    0         1         0   \n",
       "\n",
       "   sadness  surprise  trust Spanish Word  \n",
       "0        0         0      0       detrás  \n",
       "1        0         0      1        ábaco  \n",
       "2        1         0      0    abandonar  \n",
       "3        1         0      0   abandonado  \n",
       "4        1         1      0     abandono  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emolex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitaré las dimensiones `positivo`, `negativo` y `English Word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "anger",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "anticipation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "disgust",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fear",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "joy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sadness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "surprise",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "trust",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Spanish Word",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "de73fca7-07ef-4ba3-861b-7b16572a34ff",
       "rows": [
        [
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "detrás"
        ],
        [
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "ábaco"
        ],
        [
         "2",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "abandonar"
        ],
        [
         "3",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "abandonado"
        ],
        [
         "4",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1",
         "1",
         "0",
         "abandono"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>Spanish Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>detrás</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ábaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>abandonar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>abandonado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>abandono</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anger  anticipation  disgust  fear  joy  sadness  surprise  trust  \\\n",
       "0      0             0        0     0    0        0         0      0   \n",
       "1      0             0        0     0    0        0         0      1   \n",
       "2      0             0        0     1    0        1         0      0   \n",
       "3      1             0        0     1    0        1         0      0   \n",
       "4      1             0        0     1    0        1         1      0   \n",
       "\n",
       "  Spanish Word  \n",
       "0       detrás  \n",
       "1        ábaco  \n",
       "2    abandonar  \n",
       "3   abandonado  \n",
       "4     abandono  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_emolex = emolex.drop(columns=[\"English Word\", \"positive\", \"negative\"])\n",
    "clean_emolex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo la propuesta de enmascarar cada término con su emoción, pienso representar cada tweet las contribuciones de sus palabras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones para los diferentes esquemas de pesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearBoE_frecuencia(tr_txt, emolex):\n",
    "    BoE = np.zeros(shape=(len(tr_txt), 8))\n",
    "    \n",
    "    for i, tweet in enumerate(tr_txt):\n",
    "        vector = np.zeros(8)\n",
    "        palabras = tokenizer.tokenize(tweet.lower())\n",
    "        \n",
    "        # Obtener solo palabras en el lexicon\n",
    "        ocurrencias = emolex[emolex[\"Spanish Word\"].isin(palabras)]\n",
    "        \n",
    "        if not ocurrencias.empty:\n",
    "            # De forma frecuencia\n",
    "            vector = ocurrencias.drop(columns=\"Spanish Word\").sum(axis=0).to_numpy()\n",
    "    \n",
    "        BoE[i] = vector\n",
    "        \n",
    "    return BoE\n",
    "\n",
    "\n",
    "def crearBoE_binaria(tr_txt, emolex):\n",
    "    # De frecuencia transformamos a binaria\n",
    "    BoE = crearBoE_frecuencia(tr_txt, emolex)\n",
    "    BoE = np.where(BoE > 0, 1, 0)\n",
    "    return BoE\n",
    "\n",
    "\n",
    "def crearBoE_tfidf(tr_txt, emolex):\n",
    "    # De forma muy similar a BoW\n",
    "    \n",
    "    # Un BOW de frecuencia nos ayuda a obtener tf\n",
    "    # Lo haremos logaritmico\n",
    "    f_bow = crearBoE_frecuencia(tr_txt, emolex)\n",
    "    tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n",
    "    \n",
    "    # De misma forma para df\n",
    "    n_docs = len(tr_txt)\n",
    "    df = np.count_nonzero(f_bow, axis=0)\n",
    "    idf = np.log10(n_docs / (1+df))\n",
    "\n",
    "    BoE = tf*idf\n",
    "    \n",
    "    return BoE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluamos cada esquema de pesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[344  74]\n",
      " [115  54]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78       418\n",
      "           1       0.42      0.32      0.36       169\n",
      "\n",
      "    accuracy                           0.68       587\n",
      "   macro avg       0.59      0.57      0.57       587\n",
      "weighted avg       0.66      0.68      0.66       587\n",
      "\n",
      "F1-score: 0.3636\n"
     ]
    }
   ],
   "source": [
    "bag_of_emotions_train = crearBoE_binaria(tr_txt, clean_emolex)\n",
    "bag_of_emotions_test = crearBoE_binaria(val_txt, clean_emolex)\n",
    "\n",
    "svm_emolex_binario = evaluar_bow(bag_of_emotions_train, bag_of_emotions_test, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[357  61]\n",
      " [125  44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79       418\n",
      "           1       0.42      0.26      0.32       169\n",
      "\n",
      "    accuracy                           0.68       587\n",
      "   macro avg       0.58      0.56      0.56       587\n",
      "weighted avg       0.65      0.68      0.66       587\n",
      "\n",
      "F1-score: 0.3212\n"
     ]
    }
   ],
   "source": [
    "bag_of_emotions_train = crearBoE_frecuencia(tr_txt, clean_emolex)\n",
    "bag_of_emotions_test = crearBoE_frecuencia(val_txt, clean_emolex)\n",
    "\n",
    "svm_emolex_frecuencia = evaluar_bow(bag_of_emotions_train, bag_of_emotions_test, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus\\AppData\\Local\\Temp\\ipykernel_12236\\768315645.py:33: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[346  72]\n",
      " [116  53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79       418\n",
      "           1       0.42      0.31      0.36       169\n",
      "\n",
      "    accuracy                           0.68       587\n",
      "   macro avg       0.59      0.57      0.57       587\n",
      "weighted avg       0.66      0.68      0.66       587\n",
      "\n",
      "F1-score: 0.3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus\\AppData\\Local\\Temp\\ipykernel_12236\\768315645.py:33: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n"
     ]
    }
   ],
   "source": [
    "bag_of_emotions_train = crearBoE_tfidf(tr_txt, clean_emolex)\n",
    "bag_of_emotions_test = crearBoE_tfidf(val_txt, clean_emolex)\n",
    "\n",
    "svm_emolex_tfidf = evaluar_bow(bag_of_emotions_train, bag_of_emotions_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Tabla comparativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Representación  | Macro F1\n",
    "----------------|----------\n",
    "Binario         | **36.36%**\n",
    "Frecuencia      | 32.12%\n",
    "TF-IDF          | 36.05%\n",
    "\n",
    "Los resultados no son nada optimistas con mi propuesta. Si el caso fue diferente para otras propuestas me gustaría conocerlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurso Linguistico de Emociones Mexicano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Utilizar el recurso Léxico \"Spanish Emotion Lexicon\" (SEL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recurso léxico llamado \"Spanish Emotion Lexicon (SEL)\" contiene las palabras en español asociadas a una emoción básica. Estas emociones pueden ser: *Alegría*, *Enojo*, *Miedo*, *Repulsión*, *Sorpresa*, *Tristeza*. \n",
    "\n",
    "También contiene una métrica llamada **Probability of Affective Use (PFA)** la cual refleja la probabilidad de que la palabra se utilice en asociación con la emoción ligada.\n",
    "\n",
    "Mi propuesta es algo similar al recurso léxico EmoLex. Representaré a los términos como un vector $\\vec{v}$ de emociones que tendrá en cada componente i-esima el valor $PFA$ correspondiente a la i-esima emoción. $\\vec{v_i} = {PFA}_i$\n",
    "\n",
    "* Para pesado de frecuencia.  \n",
    "    Representaría a cada documento como la suma de los vectores de cada palabra que contenga. \n",
    "\n",
    "    $$\\vec{d}_k = \\sum_{v \\in d_k} \\vec{v}$$\n",
    "\n",
    "    También normalizaré para que la suma de cada componente del documento sea igual a 1.\n",
    "\n",
    "    $$ \\sum_{j} {d_k}_j = 1  $$\n",
    "\n",
    "* Para pesado binario.  \n",
    "    Utilizaré el resultado del pesado frecuencia, simplemente aplicar algo similar a la función *ceil* para cada componente.\n",
    "\n",
    "* Para pesado tfidf.  \n",
    "    Transformaré a partir de la representación tf a tfidf, similar a EmoLex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_path = \"./data/SEL.txt\"\n",
    "sel = pd.read_csv(sel_path, sep=\"\\t\")\n",
    "sel.drop(columns=[\"#\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Palabra",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": " Nula[%]",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " Baja[%] ",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " Media[%]",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " Alta[%]",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " PFA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Categoría",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "7c559459-2241-4ab9-b424-be3e6eb85a69",
       "rows": [
        [
         "0",
         "abundancia",
         "0",
         "0",
         "50",
         "50",
         "0.83",
         "Alegría"
        ],
        [
         "1",
         "acabalar",
         "40",
         "0",
         "60",
         "0",
         "0.396",
         "Alegría"
        ],
        [
         "2",
         "acallar",
         "50",
         "40",
         "10",
         "0",
         "0.198",
         "Alegría"
        ],
        [
         "3",
         "acatar",
         "50",
         "40",
         "10",
         "0",
         "0.198",
         "Alegría"
        ],
        [
         "4",
         "acción",
         "30",
         "30",
         "30",
         "10",
         "0.397",
         "Alegría"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Nula[%]</th>\n",
       "      <th>Baja[%]</th>\n",
       "      <th>Media[%]</th>\n",
       "      <th>Alta[%]</th>\n",
       "      <th>PFA</th>\n",
       "      <th>Categoría</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abundancia</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.830</td>\n",
       "      <td>Alegría</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acabalar</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.396</td>\n",
       "      <td>Alegría</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acallar</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198</td>\n",
       "      <td>Alegría</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acatar</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198</td>\n",
       "      <td>Alegría</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acción</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>0.397</td>\n",
       "      <td>Alegría</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Palabra   Nula[%]   Baja[%]    Media[%]   Alta[%]    PFA Categoría\n",
       "0  abundancia         0          0         50        50  0.830   Alegría\n",
       "1    acabalar        40          0         60         0  0.396   Alegría\n",
       "2     acallar        50         40         10         0  0.198   Alegría\n",
       "3      acatar        50         40         10         0  0.198   Alegría\n",
       "4      acción        30         30         30        10  0.397   Alegría"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Alegría', 'Enojo', 'Miedo', 'Repulsión', 'Sorpresa', 'Tristeza'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sel[\"Categoría\"].unique()))\n",
    "sel[\"Categoría\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones para crear BoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_crearBoE_frecuencia(tr_txt, sel : pd.DataFrame):\n",
    "    \n",
    "    emociones_indice = {\n",
    "        \"Alegría\" : 0,\n",
    "        \"Enojo\" : 1,\n",
    "        \"Miedo\" : 2,\n",
    "        \"Repulsión\" : 3,\n",
    "        \"Sorpresa\" : 4,\n",
    "        \"Tristeza\" : 5\n",
    "    }\n",
    "    \n",
    "    n_emociones = len(emociones_indice)\n",
    "    \n",
    "    BoE = np.zeros(shape=(len(tr_txt), n_emociones))\n",
    "    \n",
    "    for i, tweet in enumerate(tr_txt):\n",
    "        vector = np.zeros(n_emociones)\n",
    "        palabras = tokenizer.tokenize(tweet.lower())\n",
    "        \n",
    "        # Obtener solo palabras en el lexicon\n",
    "        ocurrencias = sel[sel[\"Palabra\"].isin(palabras)]\n",
    "        \n",
    "        if not ocurrencias.empty:\n",
    "            # De forma frecuencia\n",
    "            for row in ocurrencias.itertuples():\n",
    "                # Vemos a qué componente refiere la emoción\n",
    "                # y le sumamos su PFA.\n",
    "                indice = emociones_indice[row.Categoría]\n",
    "                vector[indice] += row._6\n",
    "    \n",
    "        # Normalizado\n",
    "        BoE[i] = vector / n_emociones\n",
    "        \n",
    "    return BoE\n",
    "\n",
    "def sel_crearBoE_binaria(tr_txt, sel):\n",
    "    # De frecuencia transformamos a binaria\n",
    "    BoE = sel_crearBoE_frecuencia(tr_txt, sel)\n",
    "    BoE = np.where(BoE > 0, 1, 0)\n",
    "    return BoE\n",
    "\n",
    "\n",
    "def sel_crearBoE_tfidf(tr_txt, sel):\n",
    "    # De forma muy similar a BoW\n",
    "    \n",
    "    # Un BOW de frecuencia nos ayuda a obtener tf\n",
    "    # Lo haremos logaritmico\n",
    "    f_bow = sel_crearBoE_frecuencia(tr_txt, sel)\n",
    "    tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n",
    "    \n",
    "    # De misma forma para df\n",
    "    n_docs = len(tr_txt)\n",
    "    df = np.count_nonzero(f_bow, axis=0)\n",
    "    idf = np.log10(n_docs / (1+df))\n",
    "\n",
    "    BoE = tf*idf\n",
    "    \n",
    "    return BoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluamos cada esquema de pesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 84 334]\n",
      " [ 33 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.20      0.31       418\n",
      "           1       0.29      0.80      0.43       169\n",
      "\n",
      "    accuracy                           0.37       587\n",
      "   macro avg       0.50      0.50      0.37       587\n",
      "weighted avg       0.59      0.37      0.35       587\n",
      "\n",
      "F1-score: 0.4257\n"
     ]
    }
   ],
   "source": [
    "# Binario\n",
    "bag_of_emotions_train = sel_crearBoE_binaria(tr_txt, sel)\n",
    "bag_of_emotions_test = sel_crearBoE_binaria(val_txt, sel)\n",
    "\n",
    "svm_sel_binario = evaluar_bow(bag_of_emotions_train, bag_of_emotions_test, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 84 334]\n",
      " [ 31 138]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.20      0.32       418\n",
      "           1       0.29      0.82      0.43       169\n",
      "\n",
      "    accuracy                           0.38       587\n",
      "   macro avg       0.51      0.51      0.37       587\n",
      "weighted avg       0.60      0.38      0.35       587\n",
      "\n",
      "F1-score: 0.4306\n"
     ]
    }
   ],
   "source": [
    "# Frecuencia\n",
    "bag_of_emotions_train = sel_crearBoE_frecuencia(tr_txt, sel)\n",
    "bag_of_emotions_test = sel_crearBoE_frecuencia(val_txt, sel)\n",
    "\n",
    "svm_sel_frecuencia = evaluar_bow(bag_of_emotions_train, bag_of_emotions_test, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gus\\AppData\\Local\\Temp\\ipykernel_12236\\2882340352.py:49: RuntimeWarning: divide by zero encountered in log10\n",
      "  tf = np.where(f_bow > 0, 1 + np.log10(f_bow),0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66 352]\n",
      " [ 30 139]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.16      0.26       418\n",
      "           1       0.28      0.82      0.42       169\n",
      "\n",
      "    accuracy                           0.35       587\n",
      "   macro avg       0.49      0.49      0.34       587\n",
      "weighted avg       0.57      0.35      0.30       587\n",
      "\n",
      "F1-score: 0.4212\n"
     ]
    }
   ],
   "source": [
    "# TFIDF\n",
    "bag_of_emotions_train = sel_crearBoE_tfidf(tr_txt, sel)\n",
    "bag_of_emotions_test = sel_crearBoE_tfidf(val_txt, sel)\n",
    "\n",
    "svm_sel_tfidf = evaluar_bow(bag_of_emotions_train, bag_of_emotions_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabla comparativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representación  | Macro F1\n",
    "----------------|----------\n",
    "Binario         | 42.57%\n",
    "Frecuencia      | **43.06%**\n",
    "TF-IDF          | 42.12%\n",
    "\n",
    "En este caso, el esquema de pesado de frecuencia fue el que mejor puntaje obtuvo utilizando el recurso léxico SEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sobre la estrategía para incorporar el PFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación que utilicé fue pensando en que un determinado tweet solo podría tener varias emociones asociadas y cada una tiene su PFA. Significando que la probabilidad de que el tweet esté asociada a la i-esima emoción está dada por su i-esima componente. Es por esto que normalizo para que la suma de todas las componentes sea igual a 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ¿Podemos mejorar con Bigramas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Concatenar BoW con otra BoW con 1000 bigramas más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('usuario', 'usuario'), ('puta', 'madre')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para determinar los 1000 bigramas más frecuentes\n",
    "# utilizaré el corpus sin stopwords.\n",
    "from nltk import bigrams\n",
    "from collections import Counter\n",
    "\n",
    "lista_bigramas = list(bigrams((corpus_palabras)))\n",
    "\n",
    "freq_bigramas = nltk.FreqDist(lista_bigramas)\n",
    "\n",
    "# 1000 bigramas más comunes\n",
    "bigramas_mas_comunes = [(w1,w2) for (w1,w2), freq in freq_bigramas.most_common(1000)]\n",
    "\n",
    "bigramas_mas_comunes[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La mejor BoW que obtuve según los experimentos\n",
    "# fue con pesado binario recortando a la mitad de palabras.\n",
    "\n",
    "V_mitad = V[:int(longitud_vocabulario/2)]\n",
    "dict_indices = dict()\n",
    "cont = 0\n",
    "\n",
    "# Concatenamos\n",
    "for freq, word in V_mitad:\n",
    "    dict_indices[word] = cont\n",
    "    cont += 1\n",
    "for w1, w2 in bigramas_mas_comunes:\n",
    "    dict_indices[w1+w2] = cont\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binario_bow_bigram(tr_txt, V, bigramas, dict_indices):\n",
    "    \n",
    "    BOW = np.zeros(shape=(len(tr_txt), len(V)+len(bigramas)), dtype=int)\n",
    "    \n",
    "    cont_doc = 0\n",
    "    for tr in tr_txt:\n",
    "        palabras = tokenizer.tokenize(tr)\n",
    "        fdist_doc = nltk.FreqDist(palabras)\n",
    "        \n",
    "        # Términos\n",
    "        for word in fdist_doc:\n",
    "            # Se elige ignorar si un término no está en el vocab.\n",
    "            if word not in dict_indices:\n",
    "                continue\n",
    "            BOW[cont_doc, dict_indices[word]] = 1\n",
    "            \n",
    "        # Bigramas\n",
    "        lista_bigramas = list(bigrams((palabras)))\n",
    "        for b1,b2 in lista_bigramas:\n",
    "            # Se elige ignorar si un bigrama no está en nuestros bigramas\n",
    "            if b1+b2 not in dict_indices:\n",
    "                continue\n",
    "            BOW[cont_doc, dict_indices[b1+b2]] = 1\n",
    "        \n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[358  60]\n",
      " [ 47 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       418\n",
      "           1       0.67      0.72      0.70       169\n",
      "\n",
      "    accuracy                           0.82       587\n",
      "   macro avg       0.78      0.79      0.78       587\n",
      "weighted avg       0.82      0.82      0.82       587\n",
      "\n",
      "F1-score: 0.6952\n"
     ]
    }
   ],
   "source": [
    "# evaluamos BoW+Bigramas\n",
    "BOW_train = binario_bow_bigram(tr_txt, V_mitad, bigramas_mas_comunes, dict_indices)\n",
    "BOW_test = binario_bow_bigram(val_txt, V_mitad, bigramas_mas_comunes, dict_indices)\n",
    "svm_bow_con_bigramas = evaluar_bow(BOW_train,BOW_test, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento no mejoró, bajó con una diferencia de 0.4%. Sin embargo, se sigue manteniendo sobre los otros experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Experimento con BoE+BoW+Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos todas las bolsas: emolex, sel, bow+bigramas.\n",
    "# En particular, las que tuvieron mejor puntuación por su esquema de pesado.\n",
    "\n",
    "bag_of_emotions_train = crearBoE_binaria(tr_txt, clean_emolex)\n",
    "bag_of_emotions_test = crearBoE_binaria(val_txt, clean_emolex)\n",
    "\n",
    "sel_BOW_tr = sel_crearBoE_frecuencia(tr_txt, sel)\n",
    "sel_BOW_val = sel_crearBoE_frecuencia(val_txt, sel)\n",
    "\n",
    "bow_bigramas_tr = binario_bow_bigram(tr_txt, V_mitad, bigramas_mas_comunes, dict_indices)\n",
    "bow_bigramas_val = binario_bow_bigram(val_txt, V_mitad, bigramas_mas_comunes, dict_indices)\n",
    "\n",
    "# Los unimos con np.concatenate\n",
    "boe_bow_bigram_tr = np.concatenate([bag_of_emotions_train, sel_BOW_tr, bow_bigramas_tr], axis = 1)\n",
    "boe_bow_bigram_val = np.concatenate([bag_of_emotions_test, sel_BOW_val, bow_bigramas_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[358  60]\n",
      " [ 49 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       418\n",
      "           1       0.67      0.71      0.69       169\n",
      "\n",
      "    accuracy                           0.81       587\n",
      "   macro avg       0.77      0.78      0.78       587\n",
      "weighted avg       0.82      0.81      0.82       587\n",
      "\n",
      "F1-score: 0.6877\n"
     ]
    }
   ],
   "source": [
    "# Lo evaluamos\n",
    "svm_boe_bow_bigram = evaluar_bow(boe_bow_bigram_tr, boe_bow_bigram_val, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces menos es más.\n",
    "\n",
    "En este trabajo implementé varias formas de representar a los documentos; sus términos, bigramas, palabras asociadas a emociones e incluso la combinación de todas. Sin embargo, aunque parecieran modelos más complejos y que toman en cuenta más variables, el desempeño del modelo **no mejoraba**. La mejor puntuación fue con una **BoW simple de esquema binario** y con un vocabulario recortado a la mitad. \n",
    "\n",
    "El costo computacional también fue incrementando conforme a los experimentos. Por ejemplo, en este último experimento donde combinamos todas las Bolsas tardó alrededor de 9 segundos en crear las bolsas de training y validación, mientras que el BoW simple de esquema binario tardó menos de 2 segundos en crearlo y entrenar el SVM. Esta diferencia me hace pensar que muchas veces las ideas más complicadas y con mayores variables pueden dar resultados **iguales o hasta peores**, al menos cuando hablamos de clasificar documentos. No creo que hayan ayudado mucho en este caso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
